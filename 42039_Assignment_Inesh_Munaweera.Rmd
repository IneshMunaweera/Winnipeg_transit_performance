---
title: "Competition 42039 - Assignment (Q1 and Q3)"
author: "Inesh Munaweera"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document (generated by RMarkdown) contains answers for Question 1 and Question 3. Question 2 can be found as a separate pdf file.

# Question 1: 


## Loading required packages and data

```{r,message=FALSE,warning=FALSE}
library(data.table) # To use`fread` 
library(httr)
library(tidyverse)
library(dplyr)
library(janitor)
library(stringr)
library(lubridate)
library(leaflet)
library(ggplot2)
library(tidymodels)
```

# Automated function to import and load data from the Winnipeg Open Data Portal. 

Note: No manual data downloading required to use this function. Provide the `file_path` and number of months you want to load `num_months` when calling the function. This will load the most recent months. e.g.; if `num_months` = 3, it will load the most recent three months. 

To use a large number of months, you will need a fast internet connection and sufficient storage in your hard drive. Each month requires about 1GB of space in the hard disk after extracting the zip files and also 1.5GB RAM per month. If your computer cannot accommodate the data, you have to consider a distributed computing system such as `Apache Spark` or `DuckDB`.

```{r,message=FALSE,warning=FALSE}
get_datasets <- function(file_path, num_months) {
  data_info <- read_csv(file_path)
  
  latest_data_info <- data_info %>%
    tail(num_months)
  
  urls <- latest_data_info$URL
  
  datasets <- lapply(urls, function(url) {
    
    temp_dir <- file.path(tempdir(), "downloaded_zip_files")
    dir.create(temp_dir, recursive = TRUE, showWarnings = FALSE)
    
    
    temp_zip <- tempfile(fileext = ".zip", tmpdir = temp_dir)
    
    
    download.file(url, temp_zip, mode="wb")
    
    
    unzip(temp_zip, exdir = temp_dir)
    
    
    files <- list.files(temp_dir, pattern = "\\.csv$", full.names = TRUE)
    data <- rbindlist(lapply(files, fread))
    
    unlink(temp_zip)
    unlink(temp_dir, recursive = TRUE)
    
    return(data)
  })
  
  return(rbindlist(datasets, use.names = TRUE, fill = TRUE))
}



data <- get_datasets("Transit_On-Time_Performance_Data_Archive.csv", 3)
```

## Data cleaning 

```{r,message=FALSE,warning=FALSE}
# Note: `fread` is better than the `read.csv` function in handling large datasets. 
data <- data %>%
  mutate( `Stop Number` = as.factor(`Stop Number`),
          `Route Number` = as.factor(`Route Number`),
          Location = str_remove(Location, "POINT \\("),
          Location = str_remove(Location, "\\)"),
          lon = as.numeric(str_extract(Location, "^[^ ]+")),  
          lat = as.numeric(str_extract(Location, "[^ ]+$")),
          Deviation = - Deviation/60   )   %>%    # Transform so that the delays will be denoted by positive values and in minutes
  select(-Location)%>%
  clean_names()
  
glimpse(data,5)
```
There are 14 million of data records in the last three month.

## Exploratory Data Analysis

```{r}
str(data)
```


```{r}
cat("Total number of routes:", length(unique(data$route_number )), "\n")
cat("Total number of stops:", length(unique(data$stop_number )), "\n")
```

```{r,message=FALSE,warning=FALSE}
ggplot(data, aes(x = deviation)) +
  geom_histogram(fill = "lightblue", color = "darkblue") +
  xlab("Delay (mins)") +
  ylab("Frequency") +
  xlim(c(-10,20))
summary(data$deviation)
```
The distribution of delay is skewed to the right with average around 2.4 minutes. We see the maximum to be 357 minutes which is n extreme case. I recommend using the median as the measure of central tendency due to the skewness, but I will still use the average for the illustrative purpose from here.

# Examing outliers

```{r}
boxplot(data$deviation)
print(paste0("Percentage of ourlier using the 1.5*IQR rule :",round(length(data$deviation[data$deviation %in% boxplot.stats(data$deviation)$out])/nrow(data)*100,2)))
```
There are too many outliers (9\%) to remove when using the standard 1.5*IQR rule. Most of these should be actual observations. Hence I do not recommend removing them. This can be handled later in modelling. However, we will ignore the 5\% of observations in the tails when calculating summary measures to minimize the effect from the observations at the tails.


# Identifying the routes with largest average delay

```{r}
# Calculate average deviation for each route
mean_delay_by_route <- data %>%
  filter(deviation > 0) %>%  # Consider only the delays
  group_by(route_number,route_name,route_destination) %>%
  summarise(n_stops= length(unique(stop_number)),observations = n(),mean_delay = mean(deviation, trim = 0.05, na.rm = TRUE),sd_delay=sd(deviation, na.rm = TRUE)) %>%
  arrange(desc(mean_delay))

mean_delay_by_route[1:10,]
```
Crosstown North	towards Whellams Lane shows the largest delay.

## Busiest routes

```{r}
mean_delay_by_route_busy <- mean_delay_by_route %>% arrange(desc(observations))
print(mean_delay_by_route_busy[1:10,])
```

77	Crosstown North	to Polo Park is the most active route with most data records.

## Stop with larges average delay

```{r}
mean_delay_by_stop <- data %>%
  filter(deviation > 0) %>%  # Consider only the delays
  group_by(stop_number) %>%
  summarise(observations = n(),mean_delay = mean(deviation, trim = 0.05, na.rm = TRUE),sd_delay=sd(deviation, na.rm = TRUE)) %>%
  arrange(desc(mean_delay))%>%
  filter(observations>20) # ignoring stops with a small number of observations

mean_delay_by_stop[1:12,]
```



## Busiest stops

```{r}
mean_delay_by_stop_busy <- mean_delay_by_stop %>% arrange(desc(observations))
print(mean_delay_by_stop_busy[1:10,])
```
## Average depay at busiest 100 stops

```{r}
mean_delay_by_stop_100 <- mean_delay_by_stop %>%
  slice(1:100)

my_cols <- c("red", "orange", "blue") ##1B9E77

pal <- colorNumeric(
  palette = my_cols,
  reverse = TRUE,
  domain = mean_delay_by_stop_100$mean_delay
)

#map
leaflet(mean_delay_by_stop_100) %>%
  addTiles() %>%
  addCircleMarkers(
    ~lon, ~lat,
    radius = 6,
    color = ~pal(mean_delay),
    popup = ~paste("Stop Number:", stop_number, "<br>Mean Delay:", mean_delay),
    fillOpacity = 1,
    stroke = FALSE
  ) %>%
  addLegend(
    pal = pal,
    values = ~mean_delay,
    title = "Mean Delay (minutes)",
    opacity = 1
  )

```

St. Vital area has some of the worst stops where average delays is very high.

## Average delay by weekday

```{r}
mean_delay_by_weekday <- data %>%
  filter(deviation > 0) %>%  # Consider only the delays
  group_by(day = wday(scheduled_time,label = TRUE)) %>%
  summarise(observations = n(),mean_delay = mean(deviation,trim = 0.05, na.rm = TRUE),sd_delay=sd(deviation, na.rm = TRUE))

print(mean_delay_by_weekday)
```

Most delays happens on Thursdays on average. However, it is interesting to see low average dela on Monday.

# Hourly delay on weekdays

```{r}
mean_delay_by_hour_weekdays <- data %>%
  filter(deviation > 0,day_type=="Weekday") %>%  # Consider only the delays
  group_by(hour = hour(scheduled_time)) %>%
  summarise(observations = n(),mean_delay = mean(deviation,trim = 0.05, na.rm = TRUE),sd_delay=sd(deviation, na.rm = TRUE))

print(mean_delay_by_hour_weekdays)
```
Average delay is largest during the morning and afternoon rush hours. It's very large aroung 5 p.m.





# Question 3: Toy example

## KNN model

Below we develop a simplified version of the knn model. Here only the observations of route 635 within october 2023 were considered.

```{r}
df <- data %>%
  filter(month(scheduled_time) == 10) %>%
  filter(route_number == "635") %>% 
  filter(route_destination == "Harkness Station") %>%
  select(-c(route_number, row_id, route_name, lon, lat, route_destination)) %>%
  mutate(
         # day = as.factor(wday(scheduled_time)),
         hour = as.factor(hour(scheduled_time))) %>%
  select(-c(scheduled_time, day_type)) 
  

# data splitting
df_split <- initial_split(df, prop = 0.75, strata = deviation)
df_train <- training(df_split)
df_test <- testing(df_split)

print(df_split) 
```

The model uses only the stop number and the hour of the day to predict the delay. Those factor variables were converted into dummy variables before training. 

```{r}
# Pre-processing recipe
preprocess_recipe <- recipe(deviation ~ ., data = df_train) %>%
  step_dummy(all_nominal_predictors())


preprocess_recipe %>%
  prep() 

```

k = 10 was used as the number of neighbors. In the complete analysis k should be tuned. Cross validation can be used for parameter tuning.

```{r}

# Model specifications
knn_spec <- nearest_neighbor(weight_func = "rectangular", 
                              neighbors = 10) %>%
  set_engine("kknn") %>%
  set_mode("regression")

# Cross validation folds
# df_folds <- vfold_cv(df_train, v = 5, strata = deviation)
# print(df_vfold)

knn_wkflw <- workflow() %>%
  add_recipe(preprocess_recipe) %>%
  add_model(knn_spec)

knn_wkflw
```

The the model is fitted using the training data and tested on the test set. 

```{r}
library(kknn)
library(finetune)

set.seed(345)

knn_fit <- knn_wkflw %>%
  fit(data = df_train)

knn_fit
```


```{r}
test_results <- knn_fit %>%
  predict(df_test) %>%
  bind_cols(df_test) %>%
  metrics(truth = deviation, estimate = .pred) 
  # filter(.metric %in% c('rmse', 'mae', 'rsq'))
print(test_results)
```

Above error criteria can be used to compare models.

## Next steps: 


First, we have to collect more data including environmental data, special event dates, and traffic data. For instance, an accident or major snowfall will significantly increase the number of long delays. The roads are flooded with vehicles of Blue Bombers fans on a game night. Also, there can be systematic delays due to construction on designated locations. 

Then, we will use all that data as predictors to explain the delays. In addition, we have to add a few more parameters to address the spatial and temporal correlation among bus stops and different routes. For example, if there is an accident, the first bus to pass the scene will be delayed and this information can be used to predict the arrival of the next bus. This information be shared among different routes as well. The case is the same for snowfalls and constructions.

As I mentioned in Question 2, we cannot just simply fit one model. We must try the multiple models I mentioned in Q2 and select the best among them using some predictive error criteria. 








